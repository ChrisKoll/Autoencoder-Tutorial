{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3a5fed",
   "metadata": {},
   "source": [
    "# Internship\n",
    "\n",
    "***\n",
    "\n",
    "## h5ad\n",
    "The HDF5 file is a file format used to store two different types of objects:\n",
    " - datasets\n",
    " - groups\n",
    " \n",
    "__Datasets__\n",
    "&rarr; Array like collection of data; Works similar to the numpy arrays <br>\n",
    "__Groups__\n",
    "&rarr; Folder-like containers that store datasets / other groups; Works like a dictionary\n",
    "\n",
    "With this file format you are able to store metadata next to the acutal data.\n",
    "\n",
    "&rarr; The h5ad file derieves from the use of the python package - _anndata_\n",
    "\n",
    "***\n",
    "\n",
    "_Sources:_\n",
    " - https://docs.h5py.org/en/stable/quick.html - 09.03.2023 14:43\n",
    " - https://cran.r-project.org/web/packages/anndata/readme/README.html - 09.03.2023 14:52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ceaf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use pip to install scanpy\n",
    "'''\n",
    "from scanpy import read_h5ad\n",
    "\n",
    "# File from the Heartcellatlas (Neuronal cells)\n",
    "file = '/home/ubuntu/Documents/Projects/Autoencoder_tutorial/data/hca_heart_neuronal_raw.h5ad'\n",
    "data = read_h5ad(filename=file)\n",
    "# Expression Matrix\n",
    "# --> Interesting?\n",
    "print(data.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d7817",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "***\n",
    "\n",
    "__Questions:__\n",
    " - Activation 'relu'?\n",
    " - RepeatVecotr?\n",
    " - Optimizer 'adam'?\n",
    " - Loss 'mse'?\n",
    "\n",
    "Problems working with sequences:\n",
    " - Length of sequences can vary &rarr; Fixed inputs for NN\n",
    " - Expertise is required for extracation of features for supervised learning models\n",
    " - Models require prediction that is also a sequence\n",
    "\n",
    "__LSTM Autoencoder__: <br>\n",
    "Long Short-Term Memory networks are capable of using internal memory to remember and use information for long input sequences. They are also able to learn the complex dynamics within the temporal ordering of input sequences.\n",
    "\n",
    "_Sources:_\n",
    " - https://machinelearningmastery.com/lstm-autoencoders/ - 09.03.2023 15:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a30447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 15:24:25.233764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 15:24:26.380702: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 15:24:26.382209: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efef18a8880>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''conda install keras'''\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Input\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# Reshape into [samples, timesteps, features]\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1432f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10869648 0.20678031 0.3036579  0.40014938 0.4970964  0.5952535\n",
      " 0.6952244  0.7982948  0.90587217]\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "# Demonstrate recreation\n",
    "yhat = model.predict(sequence, verbose=0)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9208b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
